{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924fda90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Determinism env (set before Torch ops for cuBLAS determinism)\n",
    "from transformer_lens import HookedTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
    "\n",
    "\n",
    "# Repro + determinism\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "except Exception:\n",
    "    pass\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\").to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39ea960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPEND_BOS = True\n",
    "prefix = \"The doctor said that \"\n",
    "\n",
    "# Tokenize once for the main prefix; used by the sanity check\n",
    "toks_prefix = model.to_tokens(prefix, prepend_bos=PREPEND_BOS).to(device)\n",
    "\n",
    "# Token IDs for the contrastive pair\n",
    "he_id = int(model.to_tokens(\" he\", prepend_bos=False)[0, 0])\n",
    "she_id = int(model.to_tokens(\" she\", prepend_bos=False)[0, 0])\n",
    "\n",
    "# Injection and readout configuration\n",
    "INJECT_LAYER = 3\n",
    "READ_LAYER = 8\n",
    "INJECT_SITE = \"hook_resid_mid\"\n",
    "READ_SITE = \"hook_resid_post\"\n",
    "INJECT_HOOK_NAME = f\"blocks.{INJECT_LAYER}.{INJECT_SITE}\"\n",
    "READ_HOOK_NAME = f\"blocks.{READ_LAYER}.{READ_SITE}\"\n",
    "\n",
    "# Alpha sweep grid\n",
    "ALPHA_RANGE = np.array(np.arange(-1, 1, 0.1), dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af9db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_LIST = [\n",
    "    \"The doctor said that \",\n",
    "    \"The nurse mentioned that \",\n",
    "    \"The engineer argued that \",\n",
    "    \"The teacher noted that \",\n",
    "    \"The manager reported that \",\n",
    "    \"The journalist said that \",\n",
    "    \"The lawyer stated that \",\n",
    "    \"The chef remarked that \",\n",
    "    \"The professor explained that \",\n",
    "    \"The scientist observed that \",\n",
    "]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def residual_at_pronoun(prefix_text: str, pronoun: str, layer: int = INJECT_LAYER, site: str = INJECT_SITE):\n",
    "    \"\"\"Capture the residual stream at the first pronoun token position under the given prefix.\n",
    "    This function mirrors the contrastive pair convention (He vs She) used in the older experiment.\n",
    "    \"\"\"\n",
    "    toks_prefix_only = model.to_tokens(\n",
    "        prefix_text, prepend_bos=PREPEND_BOS).to(device)\n",
    "    # index of first pronoun token (prefix length)\n",
    "    pronoun_pos = toks_prefix_only.shape[1]\n",
    "\n",
    "    toks = model.to_tokens(prefix_text + pronoun,\n",
    "                           prepend_bos=PREPEND_BOS).to(device)\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    def grab(activation, hook):\n",
    "        cache[\"resid\"] = activation.detach()\n",
    "        return activation\n",
    "\n",
    "    _ = model.run_with_hooks(toks, return_type=None, stop_at_layer=layer +\n",
    "                             1, fwd_hooks=[(f\"blocks.{layer}.{site}\", grab)])\n",
    "\n",
    "    return cache[\"resid\"][0, pronoun_pos, :].clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16033d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bias_vector_contrastive(layer: int = INJECT_LAYER) -> torch.Tensor:\n",
    "    \"\"\"Compute the He–She bias direction by averaging residual differences across prefixes.\n",
    "    Returns a unit vector in the residual space at the injection site (layer/site).\n",
    "    \"\"\"\n",
    "    v_sum = torch.zeros(model.cfg.d_model, device=device)\n",
    "    for p in PREFIX_LIST:\n",
    "        r_he = residual_at_pronoun(p, \" he\", layer)\n",
    "        r_she = residual_at_pronoun(p, \" she\", layer)\n",
    "        v_sum += r_he - r_she\n",
    "    return v_sum\n",
    "\n",
    "    # / (v_sum.norm() + 1e-8) normalizing down stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6605613",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def logit_lens(tokens, read_hook_name=READ_HOOK_NAME, apply_ln_final=True):\n",
    "    cache = {}\n",
    "\n",
    "    def read_hook(act, hook):\n",
    "        cache[\"resid\"] = act.detach().clone()\n",
    "        return act\n",
    "\n",
    "    _ = model.run_with_hooks(\n",
    "        tokens, return_type=None, stop_at_layer=READ_LAYER + 1, fwd_hooks=[(read_hook_name, read_hook)])\n",
    "    local_tstar = tokens.shape[1] - 1\n",
    "    resid = cache[\"resid\"][:, local_tstar: local_tstar + 1, :]\n",
    "    if apply_ln_final:\n",
    "        resid = model.ln_final(resid)\n",
    "    return model.unembed(resid)[0, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f996291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def delta_logit_lens(prompt: str, steer_vec: torch.Tensor, alpha: float, inject_hook_name: str, read_hook_name: str, inject_layer: int, read_layer: int) -> float:\n",
    "    \"\"\"Return Δ_logit = logit(He) − logit(She) at the pronoun position under a clean logit lens.\n",
    "    Does not change model weights; only adds a steer_vec at the configured injection site during the forward pass.\n",
    "    \"\"\"\n",
    "    toks = model.to_tokens(prompt, prepend_bos=PREPEND_BOS).to(device)\n",
    "    target_pos = toks.shape[1] - 1\n",
    "    cache = {}\n",
    "\n",
    "    def steer(act, hook):\n",
    "        vec = steer_vec.to(act.device)\n",
    "        act[:, target_pos, :] = act[:, target_pos, :] + alpha * vec\n",
    "        return act\n",
    "\n",
    "    def readh(act, hook):\n",
    "        cache[\"resid\"] = act.detach().clone()\n",
    "        return act\n",
    "\n",
    "    _ = model.run_with_hooks(\n",
    "        toks, return_type=None,\n",
    "        stop_at_layer=max(inject_layer, read_layer) + 1,\n",
    "        fwd_hooks=[(inject_hook_name, steer), (read_hook_name, readh)]\n",
    "    )\n",
    "\n",
    "    resid = model.ln_final(cache[\"resid\"][:, target_pos: target_pos + 1, :])\n",
    "    logits = model.unembed(resid)[0, 0, :]\n",
    "    return float((logits[he_id] - logits[she_id]).item())\n",
    "\n",
    "\n",
    "# α=0 consistency check\n",
    "# zero_delta = delta_logit_lens(\n",
    "#     prefix, v_bias, 0.0, INJECT_HOOK_NAME, READ_HOOK_NAME, INJECT_LAYER, READ_LAYER)\n",
    "# unmod_delta = float(\n",
    "#     (logit_lens(toks_prefix)[he_id] - logit_lens(toks_prefix)[she_id]).item())\n",
    "# print({\"zero_alpha_delta\": zero_delta, \"unmodified_delta\": unmod_delta})\n",
    "# assert abs(zero_delta - unmod_delta) < 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6941b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sweep across all inject/read combinations, recomputing vectors each time ---\n",
    "def run_sweep_with_hooks(vectors, alpha_grid, prompt, inj_layer, read_layer):\n",
    "    inject_hook = f\"blocks.{inj_layer}.{INJECT_SITE}\"\n",
    "    read_hook = f\"blocks.{read_layer}.{READ_SITE}\"\n",
    "    out = {k: [] for k in vectors}\n",
    "    for a in alpha_grid:\n",
    "        a = float(a)\n",
    "        for name, vec in vectors.items():\n",
    "            out[name].append(delta_logit_lens(\n",
    "                prompt, vec, a, inject_hook, read_hook, inj_layer, read_layer))\n",
    "    return out, inject_hook, read_hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84d96aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment hyperparameters section #\n",
    "inject_layers = [0]\n",
    "read_layers = list(range(model.cfg.n_layers))\n",
    "ALPHA_RANGE = np.array(np.arange(-1, 1, 0.1), dtype=float)\n",
    "prefix = \"The doctor said that \"  # or your desired prompt\n",
    "toks = model.to_tokens(prefix, prepend_bos=PREPEND_BOS).to(device)\n",
    "pos = toks.shape[1] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326db91",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1869682519.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn vectors = {\"bias\": v_bias, \"random\": v_rand, \"orth\": v_orth}\u001b[39m\n                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Helper Build bias, random, and orth vectors for this injection layer\n",
    "def build_vectors(inj):\n",
    "    def unit(x): return x / (x.norm() + 1e-8)\n",
    "    v_bias = unit(build_bias_vector_contrastive(inj))\n",
    "    torch.manual_seed(42)  # Set seed for deterministic random vector\n",
    "    v_rand = unit(torch.randn_like(v_bias))  # random vector\n",
    "    v_orth = v_rand - (v_rand @ v_bias) / \\\n",
    "        (v_bias @ v_bias) * v_bias  # orthogonalize\n",
    "    v_orth = unit(v_orth)  # normalize\n",
    "    return {\"bias\": v_bias, \"random\": v_rand, \"orth\": v_orth}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5717ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper plotting and saving\n",
    "def plot_and_save(results, inj, read):\n",
    "    \"\"\"\n",
    "    Plot and save the results for a given injection and readout layer.\n",
    "    Args:\n",
    "        results (dict): Dictionary containing results for each vector type.\n",
    "            - 'bias': list of floats representing results from applying the bias vector\n",
    "            - 'random': list of floats representing results from applying the random vector  \n",
    "            - 'orth': list of floats representing results from applying the orthogonal vector\n",
    "    in: inj: int, injection layer\n",
    "    in: read: int, readout layer\n",
    "    \"\"\"\n",
    "    colors = {\"bias\": \"#0072B2\", \"random\": \"#D55E00\", \"orth\": \"#009E73\"}\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    for name in [\"bias\", \"random\", \"orth\"]:\n",
    "        ax.plot(ALPHA_RANGE, results[name], label=name, color=colors[name],\n",
    "                linewidth=2.5 if name == \"bias\" else 2, marker=\"o\", markersize=3)\n",
    "    ax.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_xlabel(r\"Steering coefficient $\\alpha$\", fontsize=14)\n",
    "    ax.set_ylabel(\n",
    "        r\"$\\Delta_{\\mathrm{logit}} = \\mathrm{logit}(\\text{He}) - \\mathrm{logit}(\\text{She})$\", fontsize=14)\n",
    "    ax.set_title(\n",
    "        f\"BRC | inject: L{inj}:{INJECT_SITE} → read: L{read}:{READ_SITE}\", fontsize=15, weight=\"bold\")\n",
    "    ax.legend(frameon=True, fontsize=11)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    note = f\"BOS={PREPEND_BOS}, prefix='{prefix}'\"\n",
    "    ax.text(0.01, -0.14, note, transform=ax.transAxes,\n",
    "            fontsize=10, color=\"gray\")\n",
    "    # Zoom in y-axis based on bias and orth\n",
    "    yvals = np.array(results[\"bias\"] + results[\"orth\"])\n",
    "    ymin = min(min(results[\"bias\"]), min(results[\"orth\"]))\n",
    "    ymax = max(max(results[\"bias\"]), max(results[\"orth\"]))\n",
    "    yrange = ymax - ymin\n",
    "    ax.set_ylim(ymin - 0.1 * yrange, ymax + 0.1 * yrange)\n",
    "    plt.tight_layout()\n",
    "    # Save to graphs/injL{inj}/brc_injL{inj}_{INJECT_SITE}_readL{read}_{READ_SITE}.png\n",
    "    out_dir = os.path.join(\"graphs\", f\"injL{inj}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    fig_path = os.path.join(\n",
    "        out_dir, f\"brc_injL{inj}_{INJECT_SITE}_readL{read}_{READ_SITE}.png\")\n",
    "    plt.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved:\", fig_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d2f1d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read <= inj:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip reading before injecting\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results, inject_hook, read_hook = \u001b[43mrun_sweep_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALPHA_RANGE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m plot_and_save(results, inj, read)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mrun_sweep_with_hooks\u001b[39m\u001b[34m(vectors, alpha_grid, prompt, inj_layer, read_layer)\u001b[39m\n\u001b[32m      3\u001b[39m inject_hook = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minj_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINJECT_SITE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m read_hook = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mread_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREAD_SITE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m out = \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m alpha_grid:\n\u001b[32m      7\u001b[39m     a = \u001b[38;5;28mfloat\u001b[39m(a)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# --- Main loop ---\n",
    "for inj in inject_layers:\n",
    "    # vectors for this injection layer\n",
    "    vectors = build_vectors(inj)\n",
    "    for read in read_layers:\n",
    "        if read <= inj:\n",
    "            continue  # skip reading before injecting\n",
    "\n",
    "        results, inject_hook, read_hook = run_sweep_with_hooks(\n",
    "            vectors, ALPHA_RANGE, prefix, inj, read)\n",
    "        plot_and_save(results, inj, read)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
